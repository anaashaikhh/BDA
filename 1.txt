➢ First you need to perform some prerequisites:

sudo apt-get upgrade

sudo apt-get update

sudo apt-get install openjdk-8-jdk

sudo addgroup hadoop

sudo adduser --ingroup hadoop hduser

sudo usermod -aG sudo hduser

su hduser

ssh-keygen -t rsa -P ""

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

➢ Now you need to disable IPv6. Open the /etc/sysctl.conf file and add the following lines to the end of the file and save it. (One way of opening the file is sudo nano /etc/sysctl.conf, after you add the lines you need to press Ctrl+X, Shift Y and Enter)

sudo nano /etc/sysctl.conf  - ADD AT THE END OF THE FILE: 

net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

➢ Restart the system at this point

➢ Now we download hadoop: 

cd /usr/local

sudo wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop3.3.0.tar.gz

sudo tar xzf hadoop-3.3.0.tar.gz

sudo mv hadoop-3.3.0 hadoop

sudo chown -R hduser:hadoop Hadoop

➢ Now open sudo nano $HOME/.bashrc and add the following line: 

sudo nano $HOME/.bashrc
export HADOOP_HOME=/usr/local/hadoop
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
unalias fs &> /dev/null
alias fs="hadoop fs"
unalias hls &> /dev/null
alias hls="fs -ls"
lzohead () {
 hadoop fs -cat $1 | lzop -dc | head -1000 | less
}
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:/usr/local/hadoop/sbin

➢ THEN PERFORM THE COMMANDS: 

source ~/.bashrc

cd /usr/local/hadoop/etc/Hadoop

➢ ADD THE FOLLOWING line to sudo nano hadoop-env.sh : Under get hadoop-specific env variable

sudo nano hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64

➢ RUN THE FOLLOWING COMMANDS:

sudo mkdir -p /app/hadoop/tmp

sudo chown hduser:hadoop /app/hadoop/tmp

➢ Make the following changes in sudo nano core-site.xml (this file is present in /usr/local/hadoop/etc/hadoop)
Add the following between <configuration> and </configuration>:

sudo nano core-site.xml

<property>
 <name>hadoop.tmp.dir</name>
 <value>/app/hadoop/tmp</value>
 <description>A base for other temporary directories.</description>
</property>
<property>
 <name>fs.default.name</name>
 <value>hdfs://localhost:54310</value>
 <description>The name of the default file system. A URI whose
 scheme and authority determine the FileSystem implementation. The
 uri's scheme determines the config property (fs.SCHEME.impl) naming
 the FileSystem implementation class. The uri's authority is used to
 determine the host, port, etc. for a filesystem.</description>
</property>

➢ In the file sudo nano mapred-site.xml Add the following between <configuration> and </configuration>: 

sudo nano mapred-site.xml
<property>
 <name>mapred.job.tracker</name>
 <value>localhost:54311</value>
 <description>The host and port that the MapReduce job tracker runs
 at. If "local", then jobs are run in-process as a single map
 and reduce task.
 </description>
</property>

➢ In the file sudo nano hdfs-site.xml Add the following between <configuration> and </configuration>:

sudo nano hdfs-site.xml

<property>
 <name>dfs.replication</name>
 <value>1</value>
 <description>Default block replication.
 The actual number of replications can be specified when the file is created.
 The default is used if replication is not specified in create time.
 </description>
</property>

➢ Finally, we format namenode by the following commands:

hadoop namenode -format

➢ START HADOOP:

sudo apt remove openssh-server

sudo apt install openssh-server

sudo service ssh restart

ssh localhost

ssh-keygen -t rsa -P ""

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

➢ START HADOOP SERVICES: 

/usr/local/hadoop/sbin/start-all.sh

➢ This command is to check that all hadoop services are running (6 services should appear):

jps

➢ This command is to stop hadoop services:

/usr/local/hadoop/sbin/stop-all.sh